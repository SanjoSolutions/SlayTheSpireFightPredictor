{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STSFightPredictor.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUnm7mY8pL9wA+4TEzmbft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexdriedger/SlayTheSpireFightPredictor/blob/master/STSFightPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMCGetK-kYbr",
        "colab_type": "text"
      },
      "source": [
        "# Download Data Set\n",
        "\n",
        "Grab the json data set from GitHib. There are two data sets available.\n",
        "\n",
        "1. Spire Logs data set (325 000 samples)\n",
        "1. Jorbs data set (2000 samples)\n",
        "\n",
        "### Cached Data\n",
        "\n",
        "During development the pre-processed data is saved in the vm. Enable the `cached_data` flag to use the cached data. This will fail if the the json data has not been pre-processed in the current vm or if the cached data is not manually uploaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0SbaW697MDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Set to False to use the (very small) Jorbs data set\n",
        "spire_logs_data = True\n",
        "\n",
        "cached_data = False\n",
        "\n",
        "if cached_data:\n",
        "    loaded_data = np.load('cached_comp_data.npz')\n",
        "    X = loaded_data['X']\n",
        "    Y = loaded_data['Y']\n",
        "elif spire_logs_data:\n",
        "  # Spire Logs (~300 000 samples)\n",
        "  data_urls = [\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042198.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042341.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042469.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042598.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042728.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042853.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593042968.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593043097.json',\n",
        "    'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/master/out/1593042045/data_1593043113.json'\n",
        "  ]\n",
        "\n",
        "  json_data = list()\n",
        "  for data_url in data_urls:\n",
        "    print(f'Loading {data_url}')\n",
        "    with urllib.request.urlopen(data_url) as url:\n",
        "      json_data.extend(json.loads(url.read().decode()))\n",
        "else:\n",
        "  # Jorbs data (~2000 samples)\n",
        "  data_url = 'https://raw.githubusercontent.com/alexdriedger/SlayTheSpireFightPredictor/fcaa5c1dda1bc3381eb40a9b8b94c11775341a84/out/data.json'\n",
        "  with urllib.request.urlopen(data_url) as url:\n",
        "      json_data = json.loads(url.read().decode())\n",
        "\n",
        "if cached_data is False:\n",
        "  print(f'Data has {len(json_data)} samples.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_6nEFapfCLp",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Process Data\n",
        "\n",
        "Load the json training examples and process them into vectors that can be used in a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GqvFOzkxDC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4CrDCWNaV-",
        "colab_type": "text"
      },
      "source": [
        "The json data is created from `.run` files processed in a separate python script. The json data is a list of objects with the following structure:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"cards\": [\n",
        "        \"Strike_R\",\n",
        "        \"Strike_R\",\n",
        "        \"Strike_R\",\n",
        "        \"Strike_R\",\n",
        "        \"Defend_R\",\n",
        "        \"Defend_R\",\n",
        "        \"Defend_R\",\n",
        "        \"Defend_R\",\n",
        "        \"Strike_R\",\n",
        "        \"Bash\",\n",
        "        \"AscendersBane\",\n",
        "        \"Clothesline\",\n",
        "        \"Iron Wave\",\n",
        "        \"Armaments\",\n",
        "        \"Combust\",\n",
        "        \"True Grit\"\n",
        "    ],\n",
        "    \"relics\": [\n",
        "        \"Burning Blood\"\n",
        "    ],\n",
        "    \"max_hp\": 82,\n",
        "    \"entering_hp\": 82,\n",
        "    \"character\": \"IRONCLAD\",\n",
        "    \"ascension\": 20,\n",
        "    \"enemies\": \"Lots of Slimes\",\n",
        "    \"potion_used\": false,\n",
        "    \"floor\": 5,\n",
        "    \"damage_taken\": 14\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in31BTFUe0MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Categories for one hot encoder. Categories are in alphabetical order and is the order used by OneHotEncoder\n",
        "ALL_CARDS = ['A Thousand Cuts', 'A Thousand Cuts+1', 'Accuracy', 'Accuracy+1', 'Acrobatics', 'Acrobatics+1', 'Adaptation', 'Adaptation+1', 'Adrenaline', 'Adrenaline+1', 'After Image', 'After Image+1', 'Aggregate', 'Aggregate+1', 'All For One', 'All For One+1', 'All Out Attack', 'All Out Attack+1', 'Alpha', 'Alpha+1', 'Amplify', 'Amplify+1', 'Anger', 'Anger+1', 'Apotheosis', 'Apotheosis+1', 'Armaments', 'Armaments+1', 'AscendersBane', 'Auto Shields', 'Auto Shields+1', 'Backflip', 'Backflip+1', 'Backstab', 'Backstab+1', 'Ball Lightning', 'Ball Lightning+1', 'Bandage Up', 'Bandage Up+1', 'Bane', 'Bane+1', 'Barrage', 'Barrage+1', 'Barricade', 'Barricade+1', 'Bash', 'Bash+1', 'Battle Trance', 'Battle Trance+1', 'BattleHymn', 'BattleHymn+1', 'Beam Cell', 'Beam Cell+1', 'BecomeAlmighty', 'BecomeAlmighty+1', 'Berserk', 'Berserk+1', 'Beta', 'Beta+1', 'Biased Cognition', 'Biased Cognition+1', 'Bite', 'Bite+1', 'Blade Dance', 'Blade Dance+1', 'Blasphemy', 'Blasphemy+1', 'Blind', 'Blind+1', 'Blizzard', 'Blizzard+1', 'Blood for Blood', 'Blood for Blood+1', 'Bloodletting', 'Bloodletting+1', 'Bludgeon', 'Bludgeon+1', 'Blur', 'Blur+1', 'Body Slam', 'Body Slam+1', 'BootSequence', 'BootSequence+1', 'Bouncing Flask', 'Bouncing Flask+1', 'BowlingBash', 'BowlingBash+1', 'Brilliance', 'Brilliance+1', 'Brutality', 'Brutality+1', 'Buffer', 'Buffer+1', 'Bullet Time', 'Bullet Time+1', 'Burn', 'Burn+1', 'Burning Pact', 'Burning Pact+1', 'Burst', 'Burst+1', 'Calculated Gamble', 'Calculated Gamble+1', 'Caltrops', 'Caltrops+1', 'Capacitor', 'Capacitor+1', 'Carnage', 'Carnage+1', 'CarveReality', 'CarveReality+1', 'Catalyst', 'Catalyst+1', 'Chaos', 'Chaos+1', 'Chill', 'Chill+1', 'Choke', 'Choke+1', 'Chrysalis', 'Chrysalis+1', 'Clash', 'Clash+1', 'ClearTheMind', 'ClearTheMind+1', 'Cleave', 'Cleave+1', 'Cloak And Dagger', 'Cloak And Dagger+1', 'Clothesline', 'Clothesline+1', 'Clumsy', 'Cold Snap', 'Cold Snap+1', 'Collect', 'Collect+1', 'Combust', 'Combust+1', 'Compile Driver', 'Compile Driver+1', 'Concentrate', 'Concentrate+1', 'Conclude', 'Conclude+1', 'ConjureBlade', 'ConjureBlade+1', 'Consecrate', 'Consecrate+1', 'Conserve Battery', 'Conserve Battery+1', 'Consume', 'Consume+1', 'Coolheaded', 'Coolheaded+1', 'Core Surge', 'Core Surge+1', 'Corpse Explosion', 'Corpse Explosion+1', 'Corruption', 'Corruption+1', 'Creative AI', 'Creative AI+1', 'Crescendo', 'Crescendo+1', 'Crippling Poison', 'Crippling Poison+1', 'CrushJoints', 'CrushJoints+1', 'CurseOfTheBell', 'CutThroughFate', 'CutThroughFate+1', 'Dagger Spray', 'Dagger Spray+1', 'Dagger Throw', 'Dagger Throw+1', 'Dark Embrace', 'Dark Embrace+1', 'Dark Shackles', 'Dark Shackles+1', 'Darkness', 'Darkness+1', 'Dash', 'Dash+1', 'Dazed', 'Dazed+1', 'Deadly Poison', 'Deadly Poison+1', 'Decay', 'DeceiveReality', 'DeceiveReality+1', 'Deep Breath', 'Deep Breath+1', 'Defend', 'Defend+1', 'Deflect', 'Deflect+1', 'Defragment', 'Defragment+1', 'Demon Form', 'Demon Form+1', 'DeusExMachina', 'DeusExMachina+1', 'DevaForm', 'DevaForm+1', 'Devotion', 'Devotion+1', 'Die Die Die', 'Die Die Die+1', 'Disarm', 'Disarm+1', 'Discovery', 'Discovery+1', 'Distraction', 'Distraction+1', 'Dodge and Roll', 'Dodge and Roll+1', 'Doom and Gloom', 'Doom and Gloom+1', 'Doppelganger', 'Doppelganger+1', 'Double Energy', 'Double Energy+1', 'Double Tap', 'Double Tap+1', 'Doubt', 'Dramatic Entrance', 'Dramatic Entrance+1', 'Dropkick', 'Dropkick+1', 'Dual Wield', 'Dual Wield+1', 'Dualcast', 'Dualcast+1', 'Echo Form', 'Echo Form+1', 'Electrodynamics', 'Electrodynamics+1', 'EmptyBody', 'EmptyBody+1', 'EmptyFist', 'EmptyFist+1', 'EmptyMind', 'EmptyMind+1', 'Endless Agony', 'Endless Agony+1', 'Enlightenment', 'Enlightenment+1', 'Entrench', 'Entrench+1', 'Envenom', 'Envenom+1', 'Eruption', 'Eruption+1', 'Escape Plan', 'Escape Plan+1', 'Establishment', 'Establishment+1', 'Evaluate', 'Evaluate+1', 'Eviscerate', 'Eviscerate+1', 'Evolve', 'Evolve+1', 'Exhume', 'Exhume+1', 'Expertise', 'Expertise+1', 'Expunger', 'Expunger+1', 'FTL', 'FTL+1', 'FameAndFortune', 'FameAndFortune+1', 'Fasting2', 'Fasting2+1', 'FearNoEvil', 'FearNoEvil+1', 'Feed', 'Feed+1', 'Feel No Pain', 'Feel No Pain+1', 'Fiend Fire', 'Fiend Fire+1', 'Finesse', 'Finesse+1', 'Finisher', 'Finisher+1', 'Fire Breathing', 'Fire Breathing+1', 'Fission', 'Fission+1', 'Flame Barrier', 'Flame Barrier+1', 'Flash of Steel', 'Flash of Steel+1', 'Flechettes', 'Flechettes+1', 'Flex', 'Flex+1', 'FlurryOfBlows', 'FlurryOfBlows+1', 'Flying Knee', 'Flying Knee+1', 'FlyingSleeves', 'FlyingSleeves+1', 'FollowUp', 'FollowUp+1', 'Footwork', 'Footwork+1', 'Force Field', 'Force Field+1', 'ForeignInfluence', 'ForeignInfluence+1', 'Forethought', 'Forethought+1', 'Fusion', 'Fusion+1', 'Gash', 'Gash+1', 'Genetic Algorithm', 'Genetic Algorithm+1', 'Ghostly', 'Ghostly Armor', 'Ghostly Armor+1', 'Ghostly+1', 'Glacier', 'Glacier+1', 'Glass Knife', 'Glass Knife+1', 'Go for the Eyes', 'Go for the Eyes+1', 'Good Instincts', 'Good Instincts+1', 'Grand Finale', 'Grand Finale+1', 'Halt', 'Halt+1', 'HandOfGreed', 'HandOfGreed+1', 'Havoc', 'Havoc+1', 'Headbutt', 'Headbutt+1', 'Heatsinks', 'Heatsinks+1', 'Heavy Blade', 'Heavy Blade+1', 'Heel Hook', 'Heel Hook+1', 'Hello World', 'Hello World+1', 'Hemokinesis', 'Hemokinesis+1', 'Hologram', 'Hologram+1', 'Hyperbeam', 'Hyperbeam+1', 'Immolate', 'Immolate+1', 'Impatience', 'Impatience+1', 'Impervious', 'Impervious+1', 'Indignation', 'Indignation+1', 'Infernal Blade', 'Infernal Blade+1', 'Infinite Blades', 'Infinite Blades+1', 'Inflame', 'Inflame+1', 'Injury', 'InnerPeace', 'InnerPeace+1', 'Insight', 'Insight+1', 'Intimidate', 'Intimidate+1', 'Iron Wave', 'Iron Wave+1', 'J.A.X.', 'J.A.X.+1', 'Jack Of All Trades', 'Jack Of All Trades+1', 'Judgement', 'Judgement+1', 'Juggernaut', 'Juggernaut+1', 'JustLucky', 'JustLucky+1', 'Leap', 'Leap+1', 'Leg Sweep', 'Leg Sweep+1', 'LessonLearned', 'LessonLearned+1', 'LikeWater', 'LikeWater+1', 'Limit Break', 'Limit Break+1', 'LiveForever', 'LiveForever+1', 'Lockon', 'Lockon+1', 'Loop', 'Loop+1', 'Machine Learning', 'Machine Learning+1', 'Madness', 'Madness+1', 'Magnetism', 'Magnetism+1', 'Malaise', 'Malaise+1', 'Master of Strategy', 'Master of Strategy+1', 'MasterReality', 'MasterReality+1', 'Masterful Stab', 'Masterful Stab+1', 'Mayhem', 'Mayhem+1', 'Meditate', 'Meditate+1', 'Melter', 'Melter+1', 'MentalFortress', 'MentalFortress+1', 'Metallicize', 'Metallicize+1', 'Metamorphosis', 'Metamorphosis+1', 'Meteor Strike', 'Meteor Strike+1', 'Mind Blast', 'Mind Blast+1', 'Miracle', 'Miracle+1', 'Multi-Cast', 'Multi-Cast+1', 'Necronomicurse', 'Neutralize', 'Neutralize+1', 'Night Terror', 'Night Terror+1', 'Nirvana', 'Nirvana+1', 'Normality', 'Noxious Fumes', 'Noxious Fumes+1', 'Offering', 'Offering+1', 'Omega', 'Omega+1', 'Omniscience', 'Omniscience+1', 'Outmaneuver', 'Outmaneuver+1', 'Pain', 'Panacea', 'Panacea+1', 'Panache', 'Panache+1', 'PanicButton', 'PanicButton+1', 'Parasite', 'PathToVictory', 'PathToVictory+1', 'Perfected Strike', 'Perfected Strike+1', 'Perseverance', 'Perseverance+1', 'Phantasmal Killer', 'Phantasmal Killer+1', 'PiercingWail', 'PiercingWail+1', 'Poisoned Stab', 'Poisoned Stab+1', 'Pommel Strike', 'Pommel Strike+1', 'Power Through', 'Power Through+1', 'Pray', 'Pray+1', 'Predator', 'Predator+1', 'Prepared', 'Prepared+1', 'Pride', 'Prostrate', 'Prostrate+1', 'Protect', 'Protect+1', 'Pummel', 'Pummel+1', 'Purity', 'Purity+1', 'Quick Slash', 'Quick Slash+1', 'Rage', 'Rage+1', 'Ragnarok', 'Ragnarok+1', 'Rainbow', 'Rainbow+1', 'Rampage', 'Rampage+1', 'ReachHeaven', 'ReachHeaven+1', 'Reaper', 'Reaper+1', 'Reboot', 'Reboot+1', 'Rebound', 'Rebound+1', 'Reckless Charge', 'Reckless Charge+1', 'Recycle', 'Recycle+1', 'Redo', 'Redo+1', 'Reflex', 'Reflex+1', 'Regret', 'Reinforced Body', 'Reinforced Body+1', 'Reprogram', 'Reprogram+1', 'Riddle With Holes', 'Riddle With Holes+1', 'Rip and Tear', 'Rip and Tear+1', 'RitualDagger', 'RitualDagger+1', 'Rupture', 'Rupture+1', 'Sadistic Nature', 'Sadistic Nature+1', 'Safety', 'Safety+1', 'Sanctity', 'Sanctity+1', 'SandsOfTime', 'SandsOfTime+1', 'SashWhip', 'SashWhip+1', 'Scrape', 'Scrape+1', 'Scrawl', 'Scrawl+1', 'Searing Blow', 'Searing Blow+1', 'Second Wind', 'Second Wind+1', 'Secret Technique', 'Secret Technique+1', 'Secret Weapon', 'Secret Weapon+1', 'Seeing Red', 'Seeing Red+1', 'Seek', 'Seek+1', 'Self Repair', 'Self Repair+1', 'Sentinel', 'Sentinel+1', 'Setup', 'Setup+1', 'Sever Soul', 'Sever Soul+1', 'Shame', 'Shiv', 'Shiv+1', 'Shockwave', 'Shockwave+1', 'Shrug It Off', 'Shrug It Off+1', 'SignatureMove', 'SignatureMove+1', 'Skewer', 'Skewer+1', 'Skim', 'Skim+1', 'Slice', 'Slice+1', 'Slimed', 'Slimed+1', 'Smite', 'Smite+1', 'SpiritShield', 'SpiritShield+1', 'Spot Weakness', 'Spot Weakness+1', 'Stack', 'Stack+1', 'Static Discharge', 'Static Discharge+1', 'Steam', 'Steam Power', 'Steam Power+1', 'Steam+1', 'Storm', 'Storm of Steel', 'Storm of Steel+1', 'Storm+1', 'Streamline', 'Streamline+1', 'Strike', 'Strike+1', 'Study', 'Study+1', 'Sucker Punch', 'Sucker Punch+1', 'Sunder', 'Sunder+1', 'Survivor', 'Survivor+1', 'Sweeping Beam', 'Sweeping Beam+1', 'Swift Strike', 'Swift Strike+1', 'Swivel', 'Swivel+1', 'Sword Boomerang', 'Sword Boomerang+1', 'Tactician', 'Tactician+1', 'TalkToTheHand', 'TalkToTheHand+1', 'Tantrum', 'Tantrum+1', 'Tempest', 'Tempest+1', 'Terror', 'Terror+1', 'The Bomb', 'The Bomb+1', 'Thinking Ahead', 'Thinking Ahead+1', 'ThirdEye', 'ThirdEye+1', 'ThroughViolence', 'ThroughViolence+1', 'Thunder Strike', 'Thunder Strike+1', 'Thunderclap', 'Thunderclap+1', 'Tools of the Trade', 'Tools of the Trade+1', 'Transmutation', 'Transmutation+1', 'Trip', 'Trip+1', 'True Grit', 'True Grit+1', 'Turbo', 'Turbo+1', 'Twin Strike', 'Twin Strike+1', 'Underhanded Strike', 'Underhanded Strike+1', 'Undo', 'Undo+1', 'Unload', 'Unload+1', 'Uppercut', 'Uppercut+1', 'Vault', 'Vault+1', 'Vengeance', 'Vengeance+1', 'Venomology', 'Venomology+1', 'Vigilance', 'Vigilance+1', 'Violence', 'Violence+1', 'Void', 'Void+1', 'Wallop', 'Wallop+1', 'Warcry', 'Warcry+1', 'WaveOfTheHand', 'WaveOfTheHand+1',\n",
        "             'Weave', 'Weave+1', 'Well Laid Plans', 'Well Laid Plans+1', 'WheelKick', 'WheelKick+1', 'Whirlwind', 'Whirlwind+1', 'White Noise', 'White Noise+1', 'Wild Strike', 'Wild Strike+1', 'WindmillStrike', 'WindmillStrike+1', 'Wireheading', 'Wireheading+1', 'Wish', 'Wish+1', 'Worship', 'Worship+1', 'Wound', 'Wound+1', 'Wraith Form v2', 'Wraith Form v2+1', 'WreathOfFlame', 'WreathOfFlame+1', 'Writhe', 'Zap', 'Zap+1']\n",
        "ALL_RELICS = ['Akabeko', 'Anchor', 'Ancient Tea Set', 'Art of War', 'Astrolabe', 'Bag of Marbles', 'Bag of Preparation', 'Bird Faced Urn', 'Black Blood', 'Black Star', 'Blood Vial', 'Bloody Idol', 'Blue Candle', 'Boot', 'Bottled Flame', 'Bottled Lightning', 'Bottled Tornado', 'Brimstone', 'Bronze Scales', 'Burning Blood', 'Busted Crown', 'Cables', 'Calipers', 'Calling Bell', 'CaptainsWheel', 'Cauldron', 'Centennial Puzzle', 'CeramicFish', 'Champion Belt', \"Charon's Ashes\", 'Chemical X', 'CloakClasp', 'ClockworkSouvenir', 'Coffee Dripper', 'Cracked Core', 'CultistMask', 'Cursed Key', 'Damaru', 'Darkstone Periapt', 'DataDisk', 'Dead Branch', 'Dodecahedron', 'DollysMirror', 'Dream Catcher', 'Du-Vu Doll', 'Ectoplasm', 'Emotion Chip', 'Empty Cage', 'Enchiridion', 'Eternal Feather', 'FaceOfCleric', 'FossilizedHelix', 'Frozen Egg 2', 'Frozen Eye', 'FrozenCore', 'Fusion Hammer', 'Gambling Chip', 'Ginger', 'Girya', 'Golden Idol', 'GoldenEye', 'Gremlin Horn', 'GremlinMask', 'HandDrill', 'Happy Flower', 'HolyWater', 'HornCleat', 'HoveringKite', 'Ice Cream', 'Incense Burner', 'InkBottle', 'Inserter', 'Juzu Bracelet', 'Kunai', 'Lantern', \"Lee's Waffle\", 'Letter Opener', 'Lizard Tail', 'Magic Flower', 'Mango', 'Mark of Pain', 'Mark of the Bloom', 'Matryoshka', 'MawBank', 'MealTicket', 'Meat on the Bone', 'Medical Kit', 'Melange', 'Membership Card', 'Mercury Hourglass', 'Molten Egg 2', 'Mummified Hand', 'MutagenicStrength', 'Necronomicon', 'NeowsBlessing', \"Nilry's Codex\", 'Ninja Scroll', \"Nloth's Gift\", 'NlothsMask', 'Nuclear Battery', 'Nunchaku', 'Odd Mushroom', 'Oddly Smooth Stone', 'Old Coin', 'Omamori', 'OrangePellets', 'Orichalcum', 'Ornamental Fan', 'Orrery', \"Pandora's Box\", 'Pantograph', 'Paper Crane', 'Paper Frog', 'Peace Pipe', 'Pear', 'Pen Nib', \"Philosopher's Stone\", 'Pocketwatch', 'Potion Belt', 'Prayer Wheel', 'PreservedInsect', 'PrismaticShard', 'PureWater', 'Question Card', 'Red Mask', 'Red Skull', 'Regal Pillow', 'Ring of the Serpent', 'Ring of the Snake', 'Runic Capacitor', 'Runic Cube', 'Runic Dome', 'Runic Pyramid', 'SacredBark', 'Self Forming Clay', 'Shovel', 'Shuriken', 'Singing Bowl', 'SlaversCollar', 'Sling', 'Smiling Mask', 'Snake Skull', 'Snecko Eye', 'Sozu', 'Spirit Poop', 'SsserpentHead', 'StoneCalendar', 'Strange Spoon', 'Strawberry', 'StrikeDummy', 'Sundial', 'Symbiotic Virus', 'TeardropLocket', 'The Courier', 'The Specimen', 'TheAbacus', 'Thread and Needle', 'Tingsha', 'Tiny Chest', 'Tiny House', 'Toolbox', 'Torii', 'Tough Bandages', 'Toxic Egg 2', 'Toy Ornithopter', 'TungstenRod', 'Turnip', 'TwistedFunnel', 'Unceasing Top', 'Vajra', 'Velvet Choker', 'VioletLotus', 'War Paint', 'WarpedTongs', 'Whetstone', 'White Beast Statue', 'WingedGreaves', 'WristBlade', 'Yang']\n",
        "ALL_ENCOUNTERS = ['2 Fungi Beasts', '2 Louse', '2 Orb Walkers', '2 Thieves', '3 Byrds', '3 Cultists', '3 Darklings', '3 Louse', '3 Sentries', '3 Shapes', '4 Byrds', '4 Shapes', 'Apologetic Slime', 'Automaton', 'Awakened One', 'Blue Slaver', 'Book of Stabbing', 'Centurion and Healer', 'Champ', 'Chosen', 'Chosen and Byrds', 'Collector', 'Colosseum Nobs', 'Colosseum Slavers', 'Cultist', 'Cultist and Chosen', 'Donu and Deca', 'Exordium Thugs', 'Exordium Wildlife', 'Flame Bruiser 1 Orb', 'Flame Bruiser 2 Orb', 'Giant Head', 'Gremlin Gang', 'Gremlin Leader', 'Gremlin Nob', 'Hexaghost', 'Jaw Worm', 'Jaw Worm Horde', 'Lagavulin', 'Lagavulin Event', 'Large Slime', 'Looter', 'Lots of Slimes', 'Masked Bandits', 'Maw', 'Mind Bloom Boss Battle', 'Mysterious Sphere', 'Nemesis', 'Orb Walker', 'Red Slaver', 'Reptomancer', 'Sentry and Sphere', 'Shell Parasite', 'Shelled Parasite and Fungi', 'Shield and Spear', 'Slaver and Parasite', 'Slavers', 'Slime Boss', 'Small Slimes', 'Snake Plant', 'Snecko', 'Snecko and Mystics', 'Snecko and Mystics', 'Sphere and 2 Shapes', 'Spheric Guardian', 'Spire Growth', 'The Eyes', 'The Guardian', 'The Heart', 'The Mushroom Lair', 'Time Eater', 'Transient', 'Writhing Mass']\n",
        "ALL_CHARACTERS = ['DEFECT', 'IRONCLAD', 'THE_SILENT', 'WATCHER']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF0lHAf6yT2U",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Process with Loops\n",
        "\n",
        "Iterate through each training example and turn each field in the json data into each part of the vector needed for training.\n",
        "\n",
        "This will later be optomized into vectorized code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWNjgCWfMcsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_list(list_to_encode, category):\n",
        "  np_array = np.array(list_to_encode)\n",
        "  encoder = OneHotEncoder(categories=[category], sparse=False)\n",
        "  n_by_1 = np_array.reshape(len(np_array), 1)\n",
        "  onehot_encoded = encoder.fit_transform(n_by_1)\n",
        "  summed = np.sum(onehot_encoded, axis=0)\n",
        "  return summed\n",
        "\n",
        "def encode_single(value, category):\n",
        "  np_array = np.array([[value]])\n",
        "  encoder = OneHotEncoder(categories=[category], sparse=False)\n",
        "  onehot_encoded = encoder.fit_transform(np_array)\n",
        "  collapsed = np.sum(onehot_encoded, axis=0)\n",
        "  # inverse = encoder.inverse_transform(collapsed[np.newaxis, ...])\n",
        "  # print(np.array_equal(np_array, inverse))\n",
        "  return collapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfcBL7gwCgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generalize_strikes_and_defends(cards):\n",
        "  \"\"\"\n",
        "  Modifies any character specific Strikes and Defends (eg. Strike_R) into\n",
        "  general Strikes and Defends(Strike)\n",
        "  \"\"\"\n",
        "  for i, s in enumerate(cards):\n",
        "    if s.startswith('Strike_') or s.startswith('Defend_'):\n",
        "      cards[i] = re.sub('_.', '', s)\n",
        "\n",
        "def encode_cards(cards):\n",
        "  \"\"\"\n",
        "  Encodes a list of cards into a modified one-hot vector where each index\n",
        "  represents how many of that card are in the deck. The vector has length of\n",
        "  ALL_CARDS.\n",
        "  \"\"\"\n",
        "  cards = np.array(cards)\n",
        "  generalize_strikes_and_defends(cards)\n",
        "  return encode_list(cards, ALL_CARDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz1rMjuszNY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_relics(relics):\n",
        "  \"\"\"\n",
        "  Encodes a list of relics into a modified one-hot vector of length ALL_RELICS.\n",
        "  If the relic is present in relics, it will be represented as 1 in the returned\n",
        "  vector\n",
        "  \"\"\"\n",
        "  relics = np.array(relics)\n",
        "  return encode_list(relics, ALL_RELICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0aVZTfEzeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_encounter(encounter):\n",
        "  \"\"\"\n",
        "  Encode an encounter into a one-hot vector of length ALL_ENCOUNTERS\n",
        "  \"\"\"\n",
        "  return encode_single(encounter, ALL_ENCOUNTERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc3VwU71MEU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_character(character):\n",
        "  \"\"\"\n",
        "  Encode the chosen character into a one-hot vector of length ALL_CHARACTERS\n",
        "  \"\"\"\n",
        "  return encode_single(character, ALL_CHARACTERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZqDcufs3I9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sample_with_loop(sample):\n",
        "  \"\"\"\n",
        "  Encode a single sample into a 1D vector\n",
        "  \"\"\"\n",
        "  cards = encode_cards(sample['cards'])\n",
        "  relics = encode_relics(sample['relics'])\n",
        "  encounter = encode_encounter(sample['enemies'])\n",
        "  num_and_bool_data = np.array([sample['max_hp'], sample['entering_hp'], sample['ascension'], int(sample['potion_used'] == 'true')])\n",
        "  return np.concatenate((cards, relics, encounter, num_and_bool_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJZXF8A_BqHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Less than 10 samples of 50 000 affected by limits\n",
        "NUM_CARDS_FOR_EMB = 45\n",
        "NUM_RELICS_FOR_EMB = 25\n",
        "card_encoder = LabelEncoder()\n",
        "card_encoder.fit(ALL_CARDS)\n",
        "relic_encoder = LabelEncoder()\n",
        "relic_encoder.fit(ALL_RELICS)\n",
        "encounter_encoder = LabelEncoder()\n",
        "encounter_encoder.fit(ALL_ENCOUNTERS)\n",
        "\n",
        "def encode_sample_embedding_with_loop(sample):\n",
        "  \"\"\"\n",
        "  Encode a single sample into a 1D vector. Uses a label encoder for cards, relics, and encounters.\n",
        "  To be used with the model with embedding layers\n",
        "  \"\"\"\n",
        "  # Zap = 714 after the +1\n",
        "  cards = np.array(sample['cards'])\n",
        "  generalize_strikes_and_defends(cards)\n",
        "  enc_cards = card_encoder.transform(cards)\n",
        "  enc_cards += 1\n",
        "  enc_cards = enc_cards.reshape(1, -1)\n",
        "  enc_cards = tf.keras.preprocessing.sequence.pad_sequences(enc_cards, maxlen=NUM_CARDS_FOR_EMB, padding='post', truncating='post')\n",
        "\n",
        "  relics = np.array(sample['relics'])\n",
        "  enc_relics = relic_encoder.transform(relics)\n",
        "  enc_relics += 1\n",
        "  enc_relics = enc_relics.reshape(1, -1)\n",
        "  enc_relics = tf.keras.preprocessing.sequence.pad_sequences(enc_relics, maxlen=NUM_RELICS_FOR_EMB, padding='post', truncating='post')\n",
        "\n",
        "  encounter = np.array(sample['enemies'])\n",
        "  enc_encounter = encounter_encoder.transform([encounter])\n",
        "  enc_encounter = enc_encounter.reshape(1, -1)\n",
        "\n",
        "  num_and_bool_data = np.array([sample['max_hp'], sample['entering_hp'], sample['ascension'], int(sample['potion_used'] == 'true')], ndmin=2)\n",
        "\n",
        "  return np.concatenate((enc_cards, enc_relics, enc_encounter, num_and_bool_data), axis=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SaM_GSp3N-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set to True to test embedding experiments\n",
        "USE_EMBEDDING = False\n",
        "\n",
        "def preprocess_with_loop(data, log=True, log_freq=1000):\n",
        "  \"\"\"\n",
        "  Pre-processes the data one sample at a time using loops.\n",
        "  X has shape: (num_samples, training_vec_len)\n",
        "  Y has shape: (num_samples)\n",
        "  \"\"\"\n",
        "  processed_samples = []\n",
        "  y = []\n",
        "\n",
        "  if log:\n",
        "    print(f'Processing {len(data)} samples')\n",
        "  \n",
        "  for i, sample in enumerate(data):\n",
        "    if log and i % log_freq == 0:\n",
        "      print(f'{((i / len(data)) * 100):.1f} % complete. => {i} of {len(data)}')\n",
        "\n",
        "    if USE_EMBEDDING:\n",
        "      processed_samples.append(encode_sample_embedding_with_loop(sample))\n",
        "    else:\n",
        "      processed_samples.append(encode_sample_with_loop(sample))\n",
        "    \n",
        "    y.append(sample['damage_taken'])\n",
        "  X = np.vstack(processed_samples)\n",
        "  Y = np.array(y, dtype='float64')\n",
        "  return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-yUE1dFtzj",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning\n",
        "\n",
        "The fun of the project! 😃 Let's build and training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_lJoIQFEDxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Dropout, Concatenate, Average, Embedding, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime, os\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "# !pip install -U keras-tuner\n",
        "# import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3a5NzjAhItw",
        "colab_type": "text"
      },
      "source": [
        "## Scaling and Spliting Data\n",
        "\n",
        "Spliting the data into training and test set and scaling the features and labels to be between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJVsdCdWax50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if cached_data is False:\n",
        "  X, Y = preprocess_with_loop(json_data, log_freq=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_XSPY82uG1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale_X(X_data):\n",
        "  \"\"\"\n",
        "  Used with one hot encoded model\n",
        "  \"\"\"\n",
        "  X_copy = np.copy(X_data)\n",
        "  max_abs_scaler = MaxAbsScaler()\n",
        "  X_maxabs = max_abs_scaler.fit_transform(X_copy)\n",
        "  with open('input_scales.json', 'w') as out_file:\n",
        "    json.dump(max_abs_scaler.scale_.tolist(), out_file)\n",
        "  return X_maxabs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6dxcCpPtxwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale_Y(Y_data):\n",
        "  Y_copy = np.copy(Y_data)\n",
        "\n",
        "  # Scale Y\n",
        "  Y_copy /= 100\n",
        "\n",
        "  # To allow healing (negative damage), uncomment `Y[Y < -1] = -1` and comment out `Y[Y < 0] = 0`\n",
        "  Y_copy[Y_copy < -1] = -1 # Healing (negative damage)\n",
        "  # Y_copy[Y_copy < 0] = 0 # No healing\n",
        "\n",
        "  # Cap damage taken at 100\n",
        "  Y_copy[Y_copy > 1] = 1\n",
        "  return Y_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aP5ztxxzjeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def scale_and_split(X_data, Y_data):\n",
        "#   X_copy = np.copy(X_data)\n",
        "#   Y_copy = np.copy(Y_data)\n",
        "\n",
        "#   # Scale Y\n",
        "#   Y_copy /= 100\n",
        "\n",
        "#   # To allow healing (negative damage), uncomment `Y[Y < -1] = -1` and comment out `Y[Y < 0] = 0`\n",
        "#   Y_copy[Y_copy < -1] = -1 # Healing (negative damage)\n",
        "#   # Y_copy[Y_copy < 0] = 0 # No healing\n",
        "\n",
        "#   Y_copy[Y_copy > 1] = 1 # Cap damage taken at 100\n",
        "\n",
        "#   # Split into training and test data\n",
        "#   x_train, x_test, y_train, y_test = train_test_split(X_copy, Y_copy, test_size=0.33, shuffle=False)\n",
        "\n",
        "#   # Scale inputs and keep sparcity\n",
        "#   max_abs_scaler = MaxAbsScaler()\n",
        "#   X_train_maxabs = max_abs_scaler.fit_transform(x_train)\n",
        "#   X_test_maxabs = max_abs_scaler.transform(x_test)\n",
        "\n",
        "#   # Save the scale used to transform the inputs so the same transformation occurs on inputs used for prediction\n",
        "#   with open('input_scales.json', 'w') as out_file:\n",
        "#     json.dump(max_abs_scaler.scale_.tolist(), out_file)\n",
        "\n",
        "#   return X_train_maxabs, X_test_maxabs, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lPTuSNEhgH9",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train the Model\n",
        "\n",
        "We are using a Seqeuntial model with Dense and Dropout layers. Mean Absolute Error (MAE) is used as the loss function for two main reasons\n",
        "\n",
        "1. MAE has a better validation loss than Mean Squared Error (MSE)\n",
        "1. MAE represents the avg error on predictions. Example: \"You will lose 7 HP +/- X HP in this fight\" where X is the Mean Absolute Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3qpj7egwxeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(400, input_shape=(970,), activation='relu'),\n",
        "  tf.keras.layers.Dropout(.2),\n",
        "  tf.keras.layers.Dense(40, activation='relu'),\n",
        "  tf.keras.layers.Dropout(.1),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=.001),\n",
        "    loss='mean_absolute_error',\n",
        "    metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "\n",
        "# Tensorboard\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "X_scaled = scale_X(X)\n",
        "Y_scaled = scale_Y(Y)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size=0.33, shuffle=False)\n",
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_split=0.2, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0bK-_jVZGEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scores = model.evaluate(X_test, Y_test, verbose=2)\n",
        "\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fvZvGAjJ1O",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "### Jorbs Model Stats\n",
        "\n",
        "- Training loss = 0.0718\n",
        "- Validation loss = 0.0767\n",
        "- **Test loss = .0878**\n",
        "\n",
        "This means that the model is on average +/- 9 HP of the fight outcome.\n",
        "\n",
        "### Spire Logs Stats\n",
        "\n",
        "- Training loss = 0.0584\n",
        "- Validation loss = 0.0705\n",
        "- **Test loss = 0.0679**\n",
        "\n",
        "This means that the model is on average +/- 7 HP of the fight outcome.\n",
        "\n",
        "The bigger data set has a 23% better test loss! What's interesting is that the model trained on Spire Logs data has a better test loss on Jorbs data (0.0758) than a model trained only on Jorbs data (0.0878). \n",
        "\n",
        "### Things to Keep in Mind\n",
        "\n",
        "1. The data is inherently noisy. The same fight in the game could have many different outcomes, so this is a problem that needs a lot of data to make up for the noise.\n",
        "\n",
        "1. For the Jorbs model, there is not enough data (only 2000 training examples for 975 parameters) to make up for the noise in the data.\n",
        "\n",
        "1. The model trained on Jorbs data predicts values close to 0 a lot. This is probably due to Jorbs being a very good player and can take very little damage (or heal) in a lot of fights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zen-W27CYH_B",
        "colab_type": "text"
      },
      "source": [
        "### Tensorboard\n",
        "\n",
        "Inspect training and validation results with tenorboard and upload logs to tensorboard.dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk60wxTWQM0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bh7qBkRnWzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !tensorboard dev upload --logdir ./logs \\\n",
        "#   --name \"Slay the Spire fight predictions\" \\\n",
        "#   --description \"Training results predicting health loss in a Slay the Spire fight\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USd4CSibYjQ7",
        "colab_type": "text"
      },
      "source": [
        "### Manual Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Nga2iI2nnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inspect_test_cases():\n",
        "  \"\"\"\n",
        "  For manually looking at test predictions vs actual number\n",
        "  \"\"\"\n",
        "  num_test = 50\n",
        "  print(X_test.shape)\n",
        "  pred = model.predict(X_test[:num_test])\n",
        "  pred_actual = Y_test[:num_test]\n",
        "  pred_actual = pred_actual[..., np.newaxis]\n",
        "  out = np.concatenate((pred, pred_actual), axis=1)\n",
        "  np.set_printoptions(precision=3, suppress=True)\n",
        "  print(out)\n",
        "inspect_test_cases()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysor4liQUGBM",
        "colab_type": "text"
      },
      "source": [
        "## Tune the Model\n",
        "\n",
        "Let's try to find the best hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP_XrW4Hvp0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(hp):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  for i in range(hp.Int('num_layers_big', 1, 2)):\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=hp.Int('units_' + str(i), min_value=100, max_value=500, step=50, default=400), activation='relu'))\n",
        "    tf.keras.layers.Dropout(\n",
        "      hp.Float('dropout', 0.1, 0.4, step=0.1, default=0.2))\n",
        "  for i in range(hp.Int('num_layers_small', 1, 2)):\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=hp.Int('units_' + str(i), min_value=16, max_value=128, step=16, default=32), activation='relu'))\n",
        "    tf.keras.layers.Dropout(\n",
        "      hp.Float('dropout', 0, 0.3, step=0.1, default=0.2))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.RMSprop(hp.Choice('learning_rate', [1e-1, 1e-2, 1e-3, 1e-4])),\n",
        "      loss='mean_absolute_error',\n",
        "      metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "  return model\n",
        "\n",
        "def tune_model():\n",
        "  tuner = kt.Hyperband(\n",
        "      build_model,\n",
        "      objective='val_loss',\n",
        "      max_epochs=20,\n",
        "      hyperband_iterations=4)\n",
        "  tuner.search(X_train, Y_train,\n",
        "              epochs=5,\n",
        "              validation_data=(X_test, Y_test))\n",
        "  return tuner\n",
        "\n",
        "# tuner = tune_model()\n",
        "# tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm3SjKvhkmhp",
        "colab_type": "text"
      },
      "source": [
        "## Save the Model\n",
        "\n",
        "Save the model to be loaded into the game to be used in a mod!\n",
        "\n",
        "Cache the data to help speed up development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cdL-3DClRN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"STSFightPredictor\") # Saved Model Format\n",
        "# model.save(\"STSFP.h5\")\n",
        "\n",
        "np.savez_compressed('cached_comp_data.npz', X=X, Y=Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy3XtuQtVxQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For testing in the mod that the model in the mod is predicting the save values\n",
        "for i in range(3):\n",
        "  case = X_test[i]\n",
        "  sb = ''\n",
        "  sb = 'float[] testCase = {'\n",
        "  for num in case:\n",
        "    sb += str(num)\n",
        "    sb += '0f, '\n",
        "  sb += '};'\n",
        "  print(sb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j1R0um7Uwlr",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Experiments\n",
        "\n",
        "The current model has a good training loss curve but the validation curve looks more like a straight line than a curve. This problem is often related to overfit. Rather than one hot encoding cards, relics, and encounter, an experiment with Embedding layers was run to try to reduce overfit.\n",
        "\n",
        "An Embedding layer can be used to learn relations between cards. The general idea is to encode cards, relics, and enemies as vectors instead of a single numbers.\n",
        "\n",
        "Because there are a variable number of cards and relics a player can have, the average of card vectors and the average of relic vectors was taken. These averages can be passed into a Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVxFXMT9V9BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom average function to ignore masked layers\n",
        "def avg_labmda_fun(x, mask):\n",
        "  mask_cast = keras.backend.cast(mask, 'float32')\n",
        "  expanded = keras.backend.expand_dims(mask_cast)\n",
        "  count = tf.keras.backend.sum(mask_cast)\n",
        "  sum = keras.backend.sum(expanded * x, axis=1)\n",
        "  return sum / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdltqaZu5yC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_embedding_model():\n",
        "  # Embed cards and average output vectors\n",
        "  card_input = Input(shape=(NUM_CARDS_FOR_EMB, ), name='cards_input')\n",
        "  card_embedding = Embedding(len(ALL_CARDS) + 1, 26, mask_zero=True)(card_input)\n",
        "  card_average = Lambda(avg_labmda_fun, output_shape=(26, ), mask=None)(card_embedding)\n",
        "\n",
        "  # Embed relics and average output vectors\n",
        "  relic_input = Input(shape=(NUM_RELICS_FOR_EMB, ), name='relics_input')\n",
        "  relic_embedding = Embedding(len(ALL_RELICS) + 1, 13, mask_zero=True)(relic_input)\n",
        "  relic_average = Lambda(avg_labmda_fun, output_shape=(13, ), mask=None)(relic_embedding)\n",
        "\n",
        "  # Embed encounter. There is only a single encounter but the lambda is used to reshape the vector\n",
        "  encounter_input = Input(shape=(1, ), name='encounter_input')\n",
        "  encounter_embedding = Embedding(len(ALL_ENCOUNTERS), 8)(encounter_input)\n",
        "  encounter_layer_reshape = Lambda(lambda x: keras.backend.mean(x, axis=1), output_shape=(8, ))(encounter_embedding)\n",
        "\n",
        "  numbers_input = Input(shape=(4, ), name='num_and_bool_input')\n",
        "\n",
        "  # Concatenate before sending to Dense layers\n",
        "  merged = concatenate([card_average, relic_average, encounter_layer_reshape, numbers_input])\n",
        "\n",
        "  dense_1 = Dense(40, activation='relu')(merged)\n",
        "  drop_out_1 = Dropout(.1)(dense_1)\n",
        "  dense_out = Dense(1)(drop_out_1)\n",
        "\n",
        "\n",
        "  emb_model = Model(inputs=[card_input, relic_input, encounter_input, numbers_input], output=dense_out)\n",
        "  emb_model.summary()\n",
        "\n",
        "  emb_model.compile(\n",
        "      optimizer=keras.optimizers.RMSprop(learning_rate=.0001),\n",
        "      loss='mse',\n",
        "      metrics=['mae'])\n",
        "\n",
        "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  Y_scaled = scale_Y(Y)\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y_scaled, test_size=0.33, shuffle=False)\n",
        "\n",
        "  cards_col = X_train[:, 0:NUM_CARDS_FOR_EMB]\n",
        "  relic_index = NUM_CARDS_FOR_EMB + NUM_RELICS_FOR_EMB\n",
        "  relics_col = X_train[:, NUM_CARDS_FOR_EMB:relic_index]\n",
        "  encounter_index = relic_index + 1\n",
        "  encounter_col = X_train[:, relic_index:encounter_index]\n",
        "  num_and_bool_col = X_train[:, encounter_index:]\n",
        "  max_abs_scaler = MaxAbsScaler()\n",
        "  num_and_bool_col = max_abs_scaler.fit_transform(num_and_bool_col)\n",
        "\n",
        "  history = emb_model.fit(x={'cards_input': cards_col, 'relics_input': relics_col, 'encounter_input': encounter_col, 'num_and_bool_input': num_and_bool_col}, y=Y_train, batch_size=32, epochs=20, validation_split=0.2)\n",
        "  return emb_model\n",
        "\n",
        "if USE_EMBEDDING:\n",
        "  emb_model = train_embedding_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i87upB6aacyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inspect_embeddings():\n",
        "  embeddings = emb_model.layers[3].get_weights()[0]\n",
        "  embeddings.shape\n",
        "  weights = dict()\n",
        "  for i, name in enumerate(ALL_CARDS):\n",
        "    weights[name] = embeddings[i]\n",
        "    # print(f'{name}:\\t{embeddings[i]}')\n",
        "\n",
        "  print(weights['Pommel Strike'])\n",
        "  print(weights['Sucker Punch'])\n",
        "\n",
        "if USE_EMBEDDING:\n",
        "  inspect_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}